{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/ipinmi/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "from typing import List\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "\n",
    "import gensim\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download(\"brown\")  # data files for bigram collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 47  # for reproducibility\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output, save, and load brown embeddings\n",
    "\n",
    "model = gensim.models.Word2Vec(brown.sents())\n",
    "model.save(\"brown.embedding\")\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load(\"brown.embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization using Spacy\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define english preprocessing pipeline (after tokenization)\n",
    "def en_prepareText(tokens):\n",
    "    STOP_WORDS = spacy_en.Defaults.stop_words\n",
    "\n",
    "    # remove stopwords\n",
    "    tokens = [token for token in tokens if token not in STOP_WORDS]\n",
    "\n",
    "    # lemmatize the tokens\n",
    "    doc = spacy_en(\" \".join(tokens))\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# define german preprocessing pipeline (after tokenization)\n",
    "def ger_prepareText(tokens):\n",
    "    GER_STOP_WORDS = spacy_de.Defaults.stop_words\n",
    "\n",
    "    # remove stopwords\n",
    "    tokens = [token for token in tokens if token not in GER_STOP_WORDS]\n",
    "\n",
    "    # lemmatize the tokens\n",
    "    doc = spacy_de(\" \".join(tokens))\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(\n",
    "    tokenize=tokenize_de,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    lower=True,\n",
    "    # preprocessing=ger_prepareText,\n",
    ")\n",
    "TRG = Field(\n",
    "    tokenize=tokenize_en,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    lower=True,\n",
    "    # preprocessing=en_prepareText,\n",
    ")\n",
    "\n",
    "\n",
    "def loadDF(SRC, TRG):\n",
    "    \"\"\"\n",
    "\n",
    "    You will use this function to load the dataset into a Pandas Dataframe for processing.\n",
    "\n",
    "    Args:\n",
    "        split_set: the dataset split you want to load into a Pandas Dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    train_data, valid_data, test_data = Multi30k.splits(\n",
    "        exts=(\".de\", \".en\"), fields=(SRC, TRG)\n",
    "    )\n",
    "\n",
    "    return train_data, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVocab(SRC, TRG, train_dataset):\n",
    "    \"\"\"\n",
    "    Input: SRC, our list of German texts from the dataset\n",
    "            TRG, our list of English texts from the dataset\n",
    "\n",
    "    Output: SRC and TRG vocabularies\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the vocabulary for the source and target languages\n",
    "    SRC.build_vocab(train_dataset, min_freq=2)\n",
    "    TRG.build_vocab(train_dataset, min_freq=2)\n",
    "\n",
    "    # Print the number of unique tokens in the source and target vocabularies\n",
    "    print(\"Source vocabulary size:\", len(SRC.vocab))\n",
    "    print(\"Target vocabulary size:\", len(TRG.vocab))\n",
    "\n",
    "    # Print the 10 most common tokens in the source vocabulary\n",
    "    print(SRC.vocab.freqs.most_common(10))\n",
    "\n",
    "    # Print the 10 most common tokens in the target vocabulary\n",
    "    print(TRG.vocab.freqs.most_common(10))\n",
    "\n",
    "    return SRC.vocab, TRG.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(dataset, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Creating batches of data.\n",
    "    The BucketIterator will ensure that the sentences of similar length are batched together.\n",
    "\n",
    "    Input: dataset (Tuple), the dataset to split into batches\n",
    "            batch_size, the size of each batch\n",
    "\n",
    "    Output: return a batch of data with a src and trg attribute\n",
    "    \"\"\"\n",
    "    train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "        dataset, batch_size=BATCH_SIZE, device=device\n",
    "    )\n",
    "\n",
    "    return train_iterator, valid_iterator, test_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = loadDF(SRC, TRG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = (train_data, valid_data, test_data)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_batch, valid_batch, test_batch = split_into_batches(DATASET, BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - source batch\n",
    "    Layer :\n",
    "        source batch -> Embedding -> LSTM\n",
    "    Output :\n",
    "        - outputs: the top-layer hidden state for each time step\n",
    "        - LSTM hidden state: the final hidden state for each layer, stacked on top of each other\n",
    "        - LSTM cell state: the final cell state for each layer, stacked on top of each other\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    input_size : int\n",
    "        Input dimension, should equal to the source vocab size.\n",
    "\n",
    "    embd_size : int\n",
    "        Embedding layer's dimension.\n",
    "\n",
    "    hidden_size : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "\n",
    "    n_layers : int\n",
    "        Number of LSTM layers.\n",
    "\n",
    "    dropout : float\n",
    "        Dropout for the LSTM layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, embd_size, hidden_size, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.embd_size = embd_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        self.embedding = nn.Embedding(input_size, embd_size)\n",
    "\n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        self.lstm = nn.LSTM(embd_size, hidden_size, n_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------\n",
    "        src : the source vector [batch size, src length]\n",
    "        embedded: [batch size, src length,  embedding size]\n",
    "\n",
    "        Outputs\n",
    "        ------\n",
    "        Outputs: the encoder outputs from the top layer\n",
    "                [src length, batch size, hidden size * n_directions]\n",
    "        hidden: the hidden state, [n_layers * n_directions, batch size, hidden size]\n",
    "        cell: the cell state, [n_layers * n_directions, batch size, hidden size]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(src)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embd_size, hidden_size, n_layers, dropout):\n",
    "        \"\"\"\n",
    "        Input :\n",
    "            - first token in the target batch\n",
    "            - LSTM hidden state from the encoder\n",
    "            - LSTM cell state from the encoder\n",
    "        Layer :\n",
    "            target batch -> Embedding --\n",
    "                                        |\n",
    "            encoder hidden state ------ |--> LSTM -> Linear\n",
    "                                        |\n",
    "            encoder cell state   -------\n",
    "\n",
    "        Output :\n",
    "            - prediction\n",
    "            - LSTM hidden state\n",
    "            - LSTM cell state\n",
    "\n",
    "        Parmeters\n",
    "        ---------\n",
    "        output_size : int\n",
    "            Output dimension, should equal to the target vocab size.\n",
    "\n",
    "        embd_size : int\n",
    "            Embedding layer's dimension.\n",
    "\n",
    "        hidden_size : int\n",
    "            LSTM Hidden/Cell state's dimension.\n",
    "\n",
    "        n_layers : int\n",
    "            Number of LSTM layers.\n",
    "\n",
    "        dropout : float\n",
    "            Dropout for the LSTM layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.embd_size = embd_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        self.embedding = nn.Embedding(output_size, embd_size)\n",
    "\n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        self.lstm = nn.LSTM(embd_size, hidden_size, n_layers, dropout=dropout)\n",
    "\n",
    "        # self.ouput, predicts on the hidden state via a linear output layer\n",
    "        self.fcLayer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, target, hidden, cell):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target : 1D torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [batch size].\n",
    "\n",
    "        hidden, cell : 3D torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        prediction : 2D torch.LongTensor\n",
    "            For each token in the batch, the predicted target vobulary.\n",
    "            [batch size, output dim]\n",
    "\n",
    "        hidden, cell : 3D torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "\n",
    "        # [1, batch size, emb dim], the 1 serves as sent len\n",
    "        embedded = self.embedding(target.unsqueeze(0))\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "\n",
    "        prediction = self.fcLayer(outputs.squeeze(0))\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert (\n",
    "            encoder.hidden_size == decoder.hidden_size\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        trg_len, batch_size = trg.shape\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "\n",
    "        # 3D tensor to storing the decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # decoder initial hidden and cell state = last encoder's hidden and cell state\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> token\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            # inputs: input token embedding, previous hidden and previous cell states\n",
    "            # outputs: prediction, hidden state, cell state\n",
    "            prediction, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            # store the decoder result in the outputs tensor\n",
    "            outputs[t] = prediction\n",
    "\n",
    "            # applying the teacher force method based on the teacher_forcing_ratio\n",
    "            teacher_force = np.random.random() < teacher_forcing_ratio\n",
    "\n",
    "            if teacher_force:\n",
    "                # use the actual next token as the next input\n",
    "                input = trg[t]\n",
    "            else:\n",
    "                # select only the highest predicted token from the predictions\n",
    "                top1 = prediction.argmax(1)\n",
    "                input = top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocabulary size: 7853\n",
      "Target vocabulary size: 5893\n",
      "[('.', 28809), ('ein', 18851), ('einem', 13711), ('in', 11895), ('eine', 9909), (',', 8938), ('und', 8925), ('mit', 8843), ('auf', 8745), ('mann', 7805)]\n",
      "[('a', 49165), ('.', 27623), ('in', 14886), ('the', 10955), ('on', 8035), ('man', 7781), ('is', 7525), ('and', 7379), ('of', 6871), ('with', 6179)]\n"
     ]
    }
   ],
   "source": [
    "source_vocab, target_vocab = buildVocab(SRC, TRG, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjustable parameters\n",
    "INPUT_DIM = len(source_vocab)\n",
    "OUTPUT_DIM = len(target_vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HIDDEN_SIZE = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "CLIP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_SIZE, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_SIZE, N_LAYERS, DEC_DROPOUT)\n",
    "seq2seq = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fcLayer): Linear(in_features=512, out_features=5893, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13,898,501 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {model_params(seq2seq):,} trainable parameters\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.Adam(seq2seq.parameters(), lr=0.001)\n",
    "\n",
    "# Loss function\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, batch_iterator, optimizer, criterion, clip):\n",
    "    \"\"\" \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(batch_iterator):\n",
    "        # getting the source and target sentences from the batch\n",
    "        src = batch.src\n",
    "        trg = batch.trg  # [ batch size, trg len]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)  # [ batch size, trg len, output size]\n",
    "\n",
    "        # flattening the output and getting only the first column for calculating the loss\n",
    "        flatten_output = output[1:].view(\n",
    "            -1, output.shape[-1]\n",
    "        )  # [trg len * batch size, output size]\n",
    "        flatten_trg = trg[1:].view(-1)  # [trg len * batch size]\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(flatten_output, flatten_trg)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the gradient to prevent exploding gradient problem\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(batch_iterator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, batch_iterator, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    val_epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(batch_iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0)  # removing the teacher forcing\n",
    "\n",
    "            # flattening the output and getting only the first column for calculating the loss\n",
    "            flatten_output = output[1:].view(\n",
    "                -1, output.shape[-1]\n",
    "            )  # [trg len * batch size, output size]\n",
    "            flatten_trg = trg[1:].view(-1)  # [trg len * batch size]\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(flatten_output, flatten_trg)\n",
    "\n",
    "            val_epoch_loss += loss.item()\n",
    "\n",
    "    return val_epoch_loss / len(batch_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float(\"inf\")  # initialize a best validation loss to beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 13m 43s\n",
      "\tTrain Loss: 4.998 | Train PPL: 148.117\n",
      "\t Val. Loss: 4.853 |  Val. PPL: 128.167\n",
      "Epoch: 02 | Time: 12m 58s\n",
      "\tTrain Loss: 4.407 | Train PPL:  82.017\n",
      "\t Val. Loss: 4.624 |  Val. PPL: 101.910\n",
      "Epoch: 03 | Time: 12m 58s\n",
      "\tTrain Loss: 4.078 | Train PPL:  59.005\n",
      "\t Val. Loss: 4.491 |  Val. PPL:  89.175\n",
      "Epoch: 04 | Time: 12m 54s\n",
      "\tTrain Loss: 3.808 | Train PPL:  45.065\n",
      "\t Val. Loss: 4.276 |  Val. PPL:  71.943\n",
      "Epoch: 05 | Time: 13m 5s\n",
      "\tTrain Loss: 3.572 | Train PPL:  35.577\n",
      "\t Val. Loss: 3.999 |  Val. PPL:  54.569\n",
      "Epoch: 06 | Time: 12m 51s\n",
      "\tTrain Loss: 3.363 | Train PPL:  28.877\n",
      "\t Val. Loss: 3.883 |  Val. PPL:  48.588\n",
      "Epoch: 07 | Time: 12m 40s\n",
      "\tTrain Loss: 3.177 | Train PPL:  23.969\n",
      "\t Val. Loss: 3.884 |  Val. PPL:  48.636\n",
      "Epoch: 08 | Time: 12m 39s\n",
      "\tTrain Loss: 3.037 | Train PPL:  20.834\n",
      "\t Val. Loss: 3.816 |  Val. PPL:  45.411\n",
      "Epoch: 09 | Time: 12m 45s\n",
      "\tTrain Loss: 2.900 | Train PPL:  18.182\n",
      "\t Val. Loss: 3.709 |  Val. PPL:  40.810\n",
      "Epoch: 10 | Time: 13m 8s\n",
      "\tTrain Loss: 2.779 | Train PPL:  16.109\n",
      "\t Val. Loss: 3.709 |  Val. PPL:  40.828\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(seq2seq, train_batch, optimizer, criterion, CLIP)\n",
    "\n",
    "    valid_loss = evaluate(seq2seq, valid_batch, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(seq2seq.state_dict(), \"seq2seq_model.pt\")\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.712 | Test PPL:  40.931 |\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the trained model\n",
    "seq2seq.load_state_dict(torch.load(\"seq2seq_model.pt\"))\n",
    "\n",
    "test_loss = evaluate(seq2seq, test_batch, criterion)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence:  . bieber justin wie aussehe ich dass , weiÃŸt du\n",
      "target sentence:  you know i am looking like justin bieber .\n"
     ]
    }
   ],
   "source": [
    "example_index = 28\n",
    "example = train_data.examples[example_index]\n",
    "\n",
    "\n",
    "print(\"source sentence: \", \" \".join(example.src))\n",
    "print(\"target sentence: \", \" \".join(example.trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1, 5893])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tensor = SRC.process([example.src]).to(device)\n",
    "trg_tensor = TRG.process([example.trg]).to(device)\n",
    "print(trg_tensor.shape)\n",
    "\n",
    "seq2seq.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = seq2seq(src_tensor, trg_tensor, teacher_forcing_ratio=0)\n",
    "\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> <unk> <unk> <unk> <unk> <unk> <unk> . <eos> <eos>'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_idx = outputs[1:].squeeze(1).argmax(1)\n",
    "\" \".join([TRG.vocab.itos[idx] for idx in output_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_EXM = \"ich bin keinen hunger .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "src_tensor = (\n",
    "    torch.LongTensor([SRC.vocab.stoi[word] for word in example.src])\n",
    "    .unsqueeze(1)\n",
    "    .to(device)\n",
    ")\n",
    "trg_tensor = (\n",
    "    torch.LongTensor([TRG.vocab.stoi[word] for word in example.trg])\n",
    "    .unsqueeze(1)\n",
    "    .to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = seq2seq(src_tensor, trg_tensor, 0)  # turn off teacher forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(trg_tensor.shape)\n",
    "\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_idx = prediction[1:].squeeze(1).argmax(1)\n",
    "\n",
    "full_sentence = \" \".join([TRG.vocab.itos[idx] for idx in output_idx])\n",
    "\n",
    "print(full_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
