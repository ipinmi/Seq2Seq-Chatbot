{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==1.8.0\n",
      "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 735.5 MB 9.3 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 33.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.8.0) (1.21.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.8.0) (3.7.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (4.43.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (1.25.7)\n",
      "\u001b[31mERROR: torchvision 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.8.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch, torchtext\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed torch-1.8.0 torchtext-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.8.0 torchtext==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.21.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (1.7.1)\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.7/site-packages (4.1.2)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.6.4)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from scipy) (1.21.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (0.14.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from nltk) (2022.3.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.43.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy gensim nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pip\n",
      "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools\n",
      "  Downloading setuptools-67.7.2-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 34.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel\n",
      "  Downloading wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip, setuptools, wheel\n",
      "\u001b[33m  WARNING: The scripts pip, pip3, pip3.10 and pip3.7 are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script wheel is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed pip-23.1.2 setuptools-67.7.2 wheel-0.40.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.5.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.9-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.8-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.6/126.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8 (from spacy)\n",
      "  Downloading thinc-8.1.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (914 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.3/914.3 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.9/490.9 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting pathy>=0.10.0 (from spacy)\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (4.43.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.21.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.23.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-1.10.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.11.1)\n",
      "Requirement already satisfied: setuptools in /root/.local/lib/python3.7/site-packages (from spacy) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (20.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.7.4.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy) (1.16.0)\n",
      "Collecting typing-extensions<4.5.0,>=3.7.4.1 (from spacy)\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy)\n",
      "  Downloading blis-0.7.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1 (from thinc<8.2.0,>=8.1.8->spacy)\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting click<9.0.0,>=7.1.1 (from typer<0.8.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy) (1.5.0)\n",
      "Installing collected packages: cymem, typing-extensions, spacy-loggers, spacy-legacy, murmurhash, langcodes, blis, wasabi, pydantic, preshed, click, catalogue, typer, srsly, pathy, confection, thinc, spacy\n",
      "\u001b[33m  WARNING: The script pathy is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script spacy is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0 requires torch==1.9.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed blis-0.7.9 catalogue-2.0.8 click-8.1.3 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 spacy-3.5.2 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 typer-0.7.0 typing-extensions-4.4.0 wasabi-1.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /root/.local/lib/python3.7/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.43.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.23.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.1)\n",
      "Requirement already satisfied: setuptools in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (20.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.16.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2019.11.28)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /root/.local/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /root/.local/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /root/.local/lib/python3.7/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.5.0)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting de-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /root/.local/lib/python3.7/site-packages (from de-core-news-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.43.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.21.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.23.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.11.1)\n",
      "Requirement already satisfied: setuptools in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (20.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in /root/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.16.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2019.11.28)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /root/.local/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /root/.local/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /root/.local/lib/python3.7/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.5.0)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "from typing import List\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "\n",
    "import gensim\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download(\"brown\")  # data files for bigram collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 47  # for reproducibility\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output, save, and load brown embeddings\n",
    "\n",
    "model = gensim.models.Word2Vec(brown.sents())\n",
    "model.save(\"brown.embedding\")\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load(\"brown.embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization using Spacy\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define english preprocessing pipeline (after tokenization)\n",
    "def en_prepareText(tokens):\n",
    "    STOP_WORDS = spacy_en.Defaults.stop_words\n",
    "\n",
    "    # remove stopwords\n",
    "    tokens = [token for token in tokens if token not in STOP_WORDS]\n",
    "\n",
    "    # lemmatize the tokens\n",
    "    doc = spacy_en(\" \".join(tokens))\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# define german preprocessing pipeline (after tokenization)\n",
    "def ger_prepareText(tokens):\n",
    "    GER_STOP_WORDS = spacy_de.Defaults.stop_words\n",
    "\n",
    "    # remove stopwords\n",
    "    tokens = [token for token in tokens if token not in GER_STOP_WORDS]\n",
    "\n",
    "    # lemmatize the tokens\n",
    "    doc = spacy_de(\" \".join(tokens))\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German Datafield\n",
    "SRC = Field(\n",
    "    tokenize=tokenize_de,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    # preprocessing=ger_prepareText,\n",
    ")\n",
    "# English Datafield\n",
    "TRG = Field(\n",
    "    tokenize=tokenize_en,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    # preprocessing=en_prepareText,\n",
    ")\n",
    "\n",
    "\n",
    "def loadDF(SRC, TRG):\n",
    "    \"\"\"\n",
    "\n",
    "    You will use this function to load the dataset into a Pandas Dataframe for processing.\n",
    "\n",
    "    Args:\n",
    "        split_set: the dataset split you want to load into a Pandas Dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    train_data, valid_data, test_data = Multi30k.splits(\n",
    "        exts=(\".de\", \".en\"), fields=(SRC, TRG)\n",
    "    )\n",
    "\n",
    "    return train_data, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVocab(SRC, TRG, train_dataset):\n",
    "    \"\"\"\n",
    "    Input: SRC, our list of German texts from the dataset\n",
    "            TRG, our list of English texts from the dataset\n",
    "\n",
    "    Output: SRC and TRG vocabularies\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the vocabulary for the source and target languages\n",
    "    # Selecting only words that appear more than once\n",
    "    SRC.build_vocab(train_dataset, min_freq=2)\n",
    "    TRG.build_vocab(train_dataset, min_freq=2)\n",
    "\n",
    "    # Print the number of unique tokens in the source and target vocabularies\n",
    "    print(\"Source vocabulary size:\", len(SRC.vocab))\n",
    "    print(\"Target vocabulary size:\", len(TRG.vocab))\n",
    "\n",
    "    # Print the 10 most common tokens in the source vocabulary\n",
    "    print(SRC.vocab.freqs.most_common(10))\n",
    "\n",
    "    # Print the 10 most common tokens in the target vocabulary\n",
    "    print(TRG.vocab.freqs.most_common(10))\n",
    "\n",
    "    return SRC.vocab, TRG.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(dataset, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Creating batches of data.\n",
    "    The BucketIterator will ensure that the sentences of similar length are batched together.\n",
    "\n",
    "    Input: dataset (Tuple), the dataset to split into batches\n",
    "            batch_size, the size of each batch\n",
    "\n",
    "    Output: return a batch of data with a src and trg attribute\n",
    "    \"\"\"\n",
    "    train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.src),\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    return train_iterator, valid_iterator, test_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = loadDF(SRC, TRG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = (train_data, valid_data, test_data)\n",
    "BATCH_SIZE = 64  # 128\n",
    "\n",
    "train_batch, valid_batch, test_batch = split_into_batches(DATASET, BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - source batch\n",
    "    Layer :\n",
    "        source batch -> Embedding -> LSTM\n",
    "    Output :\n",
    "        - outputs: the top-layer hidden state for each time step\n",
    "        - LSTM hidden state: the final hidden state for each layer, stacked on top of each other\n",
    "        - LSTM cell state: the final cell state for each layer, stacked on top of each other\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    input_size : int\n",
    "        Input dimension, should equal to the source vocab size.\n",
    "\n",
    "    embd_size : int\n",
    "        Embedding layer's dimension.\n",
    "\n",
    "    hidden_size : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "\n",
    "    n_layers : int\n",
    "        Number of LSTM layers.\n",
    "\n",
    "    dropout : float\n",
    "        Dropout for the LSTM layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, embd_size, hidden_size, n_layers, drop):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.embd_size = embd_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        self.embedding = nn.Embedding(input_size, embd_size)\n",
    "\n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        self.lstm = nn.LSTM(embd_size, hidden_size, n_layers, dropout=drop)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------\n",
    "        src : the source vector [batch size, src length]\n",
    "        embedded: [batch size, src length,  embedding size]\n",
    "\n",
    "        Outputs\n",
    "        ------\n",
    "        Outputs: the encoder outputs from the top layer\n",
    "                [src length, batch size, hidden size * n_directions]\n",
    "        hidden: the hidden state, [n_layers * n_directions, batch size, hidden size]\n",
    "        cell: the cell state, [n_layers * n_directions, batch size, hidden size]\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embd_size, hidden_size, output_size, n_layers, drop):\n",
    "        \"\"\"\n",
    "        Input :\n",
    "            - first token in the target batch\n",
    "            - LSTM hidden state from the encoder\n",
    "            - LSTM cell state from the encoder\n",
    "        Layer :\n",
    "            target batch -> Embedding --\n",
    "                                        |\n",
    "            encoder hidden state ------ |--> LSTM -> Linear\n",
    "                                        |\n",
    "            encoder cell state   -------\n",
    "\n",
    "        Output :\n",
    "            - prediction\n",
    "            - LSTM hidden state\n",
    "            - LSTM cell state\n",
    "\n",
    "        Parmeters\n",
    "        ---------\n",
    "        output_size : int\n",
    "            Output dimension, should equal to the target vocab size.\n",
    "\n",
    "        embd_size : int\n",
    "            Embedding layer's dimension.\n",
    "\n",
    "        hidden_size : int\n",
    "            LSTM Hidden/Cell state's dimension.\n",
    "\n",
    "        n_layers : int\n",
    "            Number of LSTM layers.\n",
    "\n",
    "        dropout : float\n",
    "            Dropout for the LSTM layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.embd_size = embd_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        self.embedding = nn.Embedding(input_size, embd_size)\n",
    "\n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        self.lstm = nn.LSTM(embd_size, hidden_size, n_layers, dropout=drop)\n",
    "\n",
    "        # self.ouput, predicts on the hidden state via a linear output layer\n",
    "        self.fcLayer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, target, hidden, cell):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target : 1D torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [batch size].\n",
    "\n",
    "        hidden, cell : 3D torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        prediction : 2D torch.LongTensor\n",
    "            For each token in the batch, the predicted target vobulary.\n",
    "            [batch size, output dim]\n",
    "\n",
    "        hidden, cell : 3D torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "\n",
    "        # [1, batch size, emb dim], the 1 serves as sent len\n",
    "        target = target.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(target))\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "\n",
    "        prediction = self.fcLayer(outputs.squeeze(0))\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Parmeters\n",
    "    ---------\n",
    "    encoder : \n",
    "        Produces the context vectors for the decoder\n",
    "    \n",
    "    decoder : \n",
    "        Produces the predicted output sentence \n",
    "        \n",
    "    device : \n",
    "        Places the tensors on the GPU if it is available else CPU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert (\n",
    "            encoder.hidden_size == decoder.hidden_size\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        trg_len, batch_size = trg.shape\n",
    "        trg_vocab_size = self.decoder.output_size  # len(TRG.vocab)\n",
    "\n",
    "        # 3D tensor to storing the decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # decoder initial hidden and cell state = last encoder's hidden and cell state\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> token\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            # inputs: input token embedding, previous hidden and previous cell states\n",
    "            # outputs: prediction, hidden state, cell state\n",
    "            prediction, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            # store the decoder result in the outputs tensor\n",
    "            outputs[t] = prediction\n",
    "\n",
    "            # applying the teacher force method based on the teacher_forcing_ratio\n",
    "            teacher_force = np.random.random() < teacher_forcing_ratio\n",
    "\n",
    "            if teacher_force:\n",
    "                # use the actual next token as the next input\n",
    "                input = trg[t]\n",
    "            else:\n",
    "                # select only the highest predicted token from the predictions\n",
    "                top1 = prediction.argmax(1)\n",
    "                input = top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocabulary size: 7853\n",
      "Target vocabulary size: 5893\n",
      "[('.', 28809), ('ein', 18851), ('einem', 13711), ('in', 11895), ('eine', 9909), (',', 8938), ('und', 8925), ('mit', 8843), ('auf', 8745), ('mann', 7805)]\n",
      "[('a', 49165), ('.', 27623), ('in', 14886), ('the', 10955), ('on', 8035), ('man', 7781), ('is', 7525), ('and', 7379), ('of', 6871), ('with', 6179)]\n"
     ]
    }
   ],
   "source": [
    "source_vocab, target_vocab = buildVocab(SRC, TRG, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjustable parameters\n",
    "INPUT_DIM_ENC = len(source_vocab)\n",
    "INPUT_DIM_DEC = len(target_vocab)\n",
    "OUTPUT_DIM = len(target_vocab)\n",
    "ENC_EMB_DIM = 300  # 256\n",
    "DEC_EMB_DIM = 300  # 256\n",
    "HIDDEN_SIZE = 1024  # 512\n",
    "N_LAYERS = 2\n",
    "\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "\n",
    "learning_rate = 0.001 #0.01\n",
    "NUM_EPOCHS = 40 #10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float(\"inf\")  # initialize a best validation loss to beat\n",
    "#PATIENCE = 0\n",
    "#MAX_PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_DIM_ENC, ENC_EMB_DIM, HIDDEN_SIZE, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = Decoder(\n",
    "    INPUT_DIM_DEC, DEC_EMB_DIM, HIDDEN_SIZE, OUTPUT_DIM, N_LAYERS, DEC_DROPOUT\n",
    ")\n",
    "seq2seq = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(7853, 300)\n",
       "    (lstm): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(5893, 300)\n",
       "    (lstm): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
       "    (fcLayer): Linear(in_features=1024, out_features=5893, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 37,820,317 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {model_params(seq2seq):,} trainable parameters\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.Adam(seq2seq.parameters(), lr=learning_rate)\n",
    "\n",
    "#Learning rate scheduler \n",
    "#scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Loss function\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, batch_iterator, optimizer, criterion, clip):\n",
    "    \"\"\" \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(batch_iterator):\n",
    "        # getting the source and target sentences from the batch\n",
    "        src = batch.src.to(device)  # [ batch size, src len]\n",
    "        trg = batch.trg.to(device)  # [ batch size, trg len]\n",
    "\n",
    "        output = model(src, trg)  # [ batch size, trg len, output size]\n",
    "\n",
    "        # flattening the output and getting only the first column for calculating the loss\n",
    "        flatten_output = output[1:].view(\n",
    "            -1, output.shape[-1]\n",
    "        )  # [trg len * batch size, output size]\n",
    "        flatten_trg = trg[1:].view(-1)  # [trg len * batch size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(flatten_output, flatten_trg)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the gradient to prevent exploding gradient problem\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(batch_iterator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, batch_iterator, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    val_epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(batch_iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0)  # removing the teacher forcing\n",
    "\n",
    "            # flattening the output and getting only the first column for calculating the loss\n",
    "            flatten_output = output[1:].view(\n",
    "                -1, output.shape[-1]\n",
    "            )  # [trg len * batch size, output size]\n",
    "            flatten_trg = trg[1:].view(-1)  # [trg len * batch size]\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(flatten_output, flatten_trg)\n",
    "\n",
    "            val_epoch_loss += loss.item()\n",
    "\n",
    "    return val_epoch_loss / len(batch_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 12s\n",
      "\tTrain Loss: 4.557 | Train PPL:  95.330\n",
      "Epoch: 02 | Time: 2m 12s\n",
      "\tTrain Loss: 3.818 | Train PPL:  45.525\n",
      "Epoch: 03 | Time: 2m 13s\n",
      "\tTrain Loss: 3.458 | Train PPL:  31.742\n",
      "Epoch: 04 | Time: 2m 13s\n",
      "\tTrain Loss: 3.200 | Train PPL:  24.525\n",
      "Epoch: 05 | Time: 2m 13s\n",
      "\tTrain Loss: 2.987 | Train PPL:  19.833\n",
      "Epoch: 06 | Time: 2m 12s\n",
      "\tTrain Loss: 2.823 | Train PPL:  16.821\n",
      "Epoch: 07 | Time: 2m 13s\n",
      "\tTrain Loss: 2.685 | Train PPL:  14.661\n",
      "Epoch: 08 | Time: 2m 13s\n",
      "\tTrain Loss: 2.542 | Train PPL:  12.706\n",
      "Epoch: 09 | Time: 2m 12s\n",
      "\tTrain Loss: 2.411 | Train PPL:  11.149\n",
      "Epoch: 10 | Time: 2m 13s\n",
      "\tTrain Loss: 2.286 | Train PPL:   9.832\n",
      "Epoch: 11 | Time: 2m 13s\n",
      "\tTrain Loss: 2.189 | Train PPL:   8.926\n",
      "Epoch: 12 | Time: 2m 13s\n",
      "\tTrain Loss: 2.083 | Train PPL:   8.026\n",
      "Epoch: 13 | Time: 2m 13s\n",
      "\tTrain Loss: 1.998 | Train PPL:   7.377\n",
      "Epoch: 14 | Time: 2m 14s\n",
      "\tTrain Loss: 1.899 | Train PPL:   6.680\n",
      "Epoch: 15 | Time: 2m 14s\n",
      "\tTrain Loss: 1.821 | Train PPL:   6.179\n",
      "Epoch: 16 | Time: 2m 12s\n",
      "\tTrain Loss: 1.727 | Train PPL:   5.623\n",
      "Epoch: 17 | Time: 2m 12s\n",
      "\tTrain Loss: 1.665 | Train PPL:   5.288\n",
      "Epoch: 18 | Time: 2m 12s\n",
      "\tTrain Loss: 1.594 | Train PPL:   4.921\n",
      "Epoch: 19 | Time: 2m 12s\n",
      "\tTrain Loss: 1.538 | Train PPL:   4.655\n",
      "Epoch: 20 | Time: 2m 12s\n",
      "\tTrain Loss: 1.477 | Train PPL:   4.378\n",
      "Epoch: 21 | Time: 2m 12s\n",
      "\tTrain Loss: 1.415 | Train PPL:   4.116\n",
      "Epoch: 22 | Time: 2m 12s\n",
      "\tTrain Loss: 1.379 | Train PPL:   3.973\n",
      "Epoch: 23 | Time: 2m 12s\n",
      "\tTrain Loss: 1.323 | Train PPL:   3.754\n",
      "Epoch: 24 | Time: 2m 12s\n",
      "\tTrain Loss: 1.282 | Train PPL:   3.605\n",
      "Epoch: 25 | Time: 2m 12s\n",
      "\tTrain Loss: 1.238 | Train PPL:   3.447\n",
      "Epoch: 26 | Time: 2m 12s\n",
      "\tTrain Loss: 1.182 | Train PPL:   3.261\n",
      "Epoch: 27 | Time: 2m 12s\n",
      "\tTrain Loss: 1.151 | Train PPL:   3.160\n",
      "Epoch: 28 | Time: 2m 12s\n",
      "\tTrain Loss: 1.111 | Train PPL:   3.037\n",
      "Epoch: 29 | Time: 2m 12s\n",
      "\tTrain Loss: 1.072 | Train PPL:   2.922\n",
      "Epoch: 30 | Time: 2m 12s\n",
      "\tTrain Loss: 1.032 | Train PPL:   2.806\n",
      "Epoch: 31 | Time: 2m 12s\n",
      "\tTrain Loss: 1.000 | Train PPL:   2.718\n",
      "Epoch: 32 | Time: 2m 12s\n",
      "\tTrain Loss: 0.959 | Train PPL:   2.609\n",
      "Epoch: 33 | Time: 2m 12s\n",
      "\tTrain Loss: 0.936 | Train PPL:   2.550\n",
      "Epoch: 34 | Time: 2m 12s\n",
      "\tTrain Loss: 0.909 | Train PPL:   2.481\n",
      "Epoch: 35 | Time: 2m 12s\n",
      "\tTrain Loss: 0.882 | Train PPL:   2.417\n",
      "Epoch: 36 | Time: 2m 12s\n",
      "\tTrain Loss: 0.867 | Train PPL:   2.381\n",
      "Epoch: 37 | Time: 2m 12s\n",
      "\tTrain Loss: 0.834 | Train PPL:   2.302\n",
      "Epoch: 38 | Time: 2m 12s\n",
      "\tTrain Loss: 0.801 | Train PPL:   2.227\n",
      "Epoch: 39 | Time: 2m 12s\n",
      "\tTrain Loss: 0.773 | Train PPL:   2.165\n",
      "Epoch: 40 | Time: 2m 12s\n",
      "\tTrain Loss: 0.763 | Train PPL:   2.145\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(seq2seq, train_batch, optimizer, criterion, CLIP)\n",
    "\n",
    "    valid_loss = evaluate(seq2seq, valid_batch, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "       best_valid_loss = valid_loss\n",
    "       torch.save(seq2seq.state_dict(), \"seq2seq_model.pt\")\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "    #print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.646 | Test PPL:  38.334 |\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the trained model\n",
    "seq2seq.load_state_dict(torch.load(\"seq2seq_model.pt\"))\n",
    "\n",
    "test_loss = evaluate(seq2seq, test_batch, criterion)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence:  . schlange der in geschäft einem in steht baseballmütze schwarzen einer mit und pulli grauen einem in frau eine\n",
      "target sentence:  a woman in a gray sweater and black baseball cap is standing in line at a shop .\n"
     ]
    }
   ],
   "source": [
    "example_index = 17\n",
    "example = test_data.examples[example_index]\n",
    "\n",
    "\n",
    "print(\"source sentence: \", \" \".join(example.src))\n",
    "print(\"target sentence: \", \" \".join(example.trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 5893])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tensor = SRC.process([example.src]).to(device)\n",
    "trg_tensor = TRG.process([example.trg]).to(device)\n",
    "print(trg_tensor.shape)\n",
    "\n",
    "seq2seq.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = seq2seq(src_tensor, trg_tensor, teacher_forcing_ratio=0)\n",
    "\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a woman in a black coat and a black hat is standing in a doorway in a city .'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_idx = outputs[1:].squeeze(1).argmax(1)\n",
    "\" \".join([TRG.vocab.itos[idx] for idx in output_idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code and Tutorial attribution\n",
    "\n",
    "- Ben Trevett tutorial on understanding torchtext library and the machine translation task. https://github.com/bentrevett/pytorch-seq2seq\n",
    "- Machine Translation using Recurrent Neural Network and PyTorch by Avinash Barnwal https://medium.com/analytics-vidhya/machine-translation-using-recurrent-neural-network-and-pytorch-implementation-5e59ca919e85"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
